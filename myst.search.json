{"version":"1","records":[{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/axtree","position":0},{"hierarchy":{"lvl1":""},"content":"https://​www​.reddit​.com​/r​/LocalLLM​/comments​/1rcencg​/using​_ax​_tree​_for​_llm​_web​_automation​_hitting/\n\n\nhttps://​github​.com​/elidickinson​/browser​-cli\n\nhttps://​www​.reddit​.com​/r​/LocalLLaMA​/comments​/1qcxllu​/i​_built​_a​_dompruning​_engine​_to​_run​_reliable/","type":"content","url":"/axtree","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":""},"content":"from google.colab import drive\ndrive.mount('/content/drive')\n\n\n\n\nfrom google.colab import userdata\nimport os\n\n# Retrieve the secret value from Colab Secrets\nngrok_key = userdata.get('NGROK_AUTH_TOKEN') # Use the name you defined in Step 1\nhyperbolic_key = userdata.get('HYPERBOLIC_API_KEY')\n# Set it as an environment variable\nos.environ[\"HYPERBOLIC_API_KEY\"] = hyperbolic_key\nos.environ[\"NGROK_AUTH_TOKEN\"] = ngrok_key\n\n\n\n\n\n!pip install -q pyngrok\n# there is no ngrok directory\n\n\n\n!ls ~/.config\n\n\n\n\n\n# .config/ngrok created when you add token\n!ngrok config add-authtoken {os.environ['NGROK_AUTH_TOKEN']}\n\n\n\n!ls ~/.config\n\n\n\n\n\nimport os\n\n# Get the token from the environment variable\n# If you are using Colab Secrets, ensure you've set NGROK_AUTH_TOKEN there first!\ntoken = os.environ.get('NGROK_AUTH_TOKEN')\n\nif not token:\n    raise ValueError(\"NGROK_AUTH_TOKEN environment variable not found!\")\n\n# Use an f-string (notice the 'f' before the triple quotes)\nconfig = f\"\"\"\nversion: \"2\"\nregion: us\nauthtoken: {token}\n\ntunnels:\n  ollama:\n    proto: http\n    addr: 11434\n    host_header: localhost:11434\n    domain: labrador-fair-trivially.ngrok-free.app\n\"\"\"\n\n# Ensure the directory exists\nos.makedirs('/root/.config/ngrok', exist_ok=True)\n\nwith open('/root/.config/ngrok/ngrok.yml', 'w') as f:\n    f.write(config)\n\nprint(\"✅ ngrok.yml updated with environment variable.\")\n\n\n\nngrok.yml\n~/.config/ngrok/ngrok.yml\n!ngrok start --all\n!curl https://labrador-fair-trivially.ngrok-free.app/api/generate -d '{ \\\n  \"model\": \"qwen2.5:1.5b\", \\\n  \"prompt\": \"Write a short poem about a cat in space.\", \\\n  \"stream\": false \\\n}'\n\n# for running in colab cell.\n%%bash\nset -e\napt-get install zstd\n# Install ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Show version\nollama --version\n\n# Start persistent server in background\n# this doesnt stay up in anotehr cell?\n# not reliable in colab but need it to download models\nnohup ollama serve > /tmp/ollama.log 2>&1 &\n\n# Wait for server\nsleep 2\n\n# Pull model (downloads if missing)\nollama pull qwen3-vl\n\n# Test API locally\ncurl http://127.0.0.1:11434/api/chat -d '{\n  \"model\": \"qwen3-vl\",\n  \"messages\": [{\"role\":\"user\",\"content\":\"Give me a 2-sentence summary of PPO.\"}],\n  \"stream\": true\n}'\n\n\n\n\n\n# can we start grok before installing and starting ollama? No because we customzied ngrok.yml for ollama\n!ngrok start --all\n\n\n\n!ollama list\n\n\n\n# Kill old ngrok\n# where is the grok start all command? This is better? does start all use the config file?\n!pkill -f ngrok || true\n\n# Start ngrok in background with explicit inspect port\n# this is a mistake it doesnt use the config file and starts\n# a tunnel with a random assigned string.\n# !nohup ngrok http 11434 \\\n#     --log=stdout \\\n#     --log-level=info \\\n#     > /tmp/ngrok.log 2>&1 &\n\n!nohup ngrok start ollama \\\n    --config=/root/.config/ngrok/ngrok.yml \\\n    --log=stdout \\\n    --log-level=info \\\n    > /tmp/ngrok.log 2>&1 &\n\n# Give it time to start\n!sleep 5\n\n# Verify ngrok process\n!ps -ef | grep -i '[n]grok'\n\n# Verify inspect API\n!curl -s http://127.0.0.1:4040/api/tunnels | python3 -m json.tool\n\n# If still nothing, print logs\n!tail -n 50 /tmp/ngrok.log\n\n\n\n%%bash\nset -e\n# replace above cell bc this tests for log file\n# 1. Kill any zombie ngrok processes\npkill -f ngrok || true\n\n# 2. Ensure the log file exists and is writable\ntouch /tmp/ngrok.log\nchmod 666 /tmp/ngrok.log\n\n# 3. Start ngrok with an absolute path\n# We'll use 'start ollama' to use your config\nnohup /usr/local/bin/ngrok start ollama \\\n    --config=/root/.config/ngrok/ngrok.yml \\\n    --log=stdout \\\n    > /tmp/ngrok.log 2>&1 &\n\n# 4. Wait for it...\nsleep 5\n\n# 5. Check if the file exists now\nif [ -f /tmp/ngrok.log ]; then\n    echo \"--- NGROK LOG OUTPUT ---\"\n    cat /tmp/ngrok.log\nelse\n    echo \"❌ Log file still not found. Check if ngrok is installed via: which ngrok\"\nfi\n\n\n\n!tail -n 50 /tmp/ngrok.log\n\n\n\n%%bash\nset -e\ncurl -sS http://127.0.0.1:4040/api/tunnels | jq .\n\n\n\n!curl http://127.0.0.1:11434/api/version\n\n\n\nimport requests\n\n# Replace with your actual ngrok URL\nNGROK_URL = \"https://labrador-fair-trivially.ngrok-free.app\"\n\ntry:\n    response = requests.get(f\"{NGROK_URL}/api/tags\")\n    if response.status_code == 200:\n        print(\"✅ Connection Successful!\")\n        print(\"Models available:\", response.json())\n    else:\n        print(f\"❌ Connection failed with status code: {response.status_code}\")\nexcept Exception as e:\n    print(f\"❌ Error: {e}\")\n\n\n\n!pip install ollama #there are 2, ollama server and ollama client. This is ollama client\nimport ollama\n\nclient = ollama.Client(host='https://labrador-fair-trivially.ngrok-free.app')\n\nstream = client.chat(\n    model='qwen3-vl',\n    messages=[{'role': 'user', 'content': 'Count from 1 to 20 slowly.'}],\n    stream=True,\n)\n\nprint(\"Assistant: \", end=\"\")\nfor chunk in stream:\n    print(chunk['message']['content'], end='', flush=True)\n\n\n\n!ollama list\n\n\n\n# converting to python colab cli version\n# the cell is running the brower gym python server http.server\nimport os\nimport subprocess\nimport time\nimport requests\n\n# 1. Set host to allow ngrok access\nos.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n\n# 2. Start Ollama as a background process\n# Using Popen is generally more robust than threading for CLI daemons\nprint(\"Starting Ollama server...\")\nwith open(\"/tmp/ollama.log\", \"w\") as f:\n    process = subprocess.Popen([\"ollama\", \"serve\"], stdout=f, stderr=f)\n\n# 3. Wait for the server to be ready\nfor i in range(10):\n    try:\n        response = requests.get(\"http://localhost:11434/api/tags\")\n        if response.status_code == 200:\n            print(\"✅ Ollama is up and running!\")\n            break\n    except:\n        time.sleep(2)\nelse:\n    print(\"❌ Ollama failed to start. Check /tmp/ollama.log\")\n\n# 4. Pull the specific Vision model\nprint(\"Ensuring Qwen VL is pulled...\")\nsubprocess.run([\"ollama\", \"pull\", \"qwen2.5-vl:3b\"])\n\n\n\n\n\n\n\n%cd /content/BrowserGym/miniwob-plusplus/miniwob/html\n#!nohup python -m http.server 8000\n#!nohup python3 -m http.server 8000 > /dev/null 2>&1 &\n!nohup python3 -m http.server 8000 > server.log 2>&1 &\n\n\n\n!tail %cd /content/BrowserGym/miniwob-plusplus/miniwob/html/server.log\n\n\n\n%%bash\nset -e\n# doesnt work from here, make sure to run this in colab terminal\nexport MINIWOB_URL=\"http://localhost:8000/miniwob/\"\n\n\n\nimport requests\n\n# This asks the local Ollama server about the models currently in memory\nresponse = requests.get(\"http://localhost:11434/api/ps\")\nprint(response.json())\n\n","type":"content","url":"/","position":1}]}